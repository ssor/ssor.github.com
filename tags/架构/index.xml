<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>架构 on 我思故我在</title>
    <link>http://ssor.github.io/tags/%E6%9E%B6%E6%9E%84/index.xml</link>
    <description>Recent content in 架构 on 我思故我在</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://ssor.github.io/tags/%E6%9E%B6%E6%9E%84/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>小书包系统架构演进</title>
      <link>http://ssor.github.io/post/architechure_design_dyxsb/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ssor.github.io/post/architechure_design_dyxsb/</guid>
      <description>&lt;p&gt;介绍小书包的架构历史及发展方向&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;系统组成&#34;&gt;系统组成&lt;/h2&gt;

&lt;p&gt;小书包整个系统的组成包含三部分:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;后台服务&lt;/li&gt;
&lt;li&gt;后台管理系统,&lt;/li&gt;
&lt;li&gt;终端应用( IOS 和 安卓)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;写下第一行代码的时候, 也是小书包背负最多紧急任务的时候. 在这之前已经开发了一个版本,  对新版本的要求是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;克隆全部第一个版本所具有的功能(即使是错的)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在之前功能的基础上, 开发更高级的功能&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是面对需求, 这个版本没有办法完成任务:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;之前的版本基于 Java 技术栈, 现在的研发没有这方面的经验, 而且剩余的时间也不够转型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;之前的代码在交接时缺少关键部分, 没法运行, 因某种原因不能与之前开发团队沟通&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;经过评估, 在原来代码的基础上进行修补和改进, 成本比重新开发还要高&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;决定重新开发后, 选择了 Go 作为后台主要开发语言, 原因如&lt;a href=&#34;http://ssor.github.io/post/why_use_golang_for_backend/&#34;&gt;这篇文章&lt;/a&gt;所说, 另外还有:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我自己对Go 开发有很长时间的经验, 遇到问题能够自行解决&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;语言简单, 有经验的程序员上手非常快&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go 的运行效率也够高&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;主服务&#34;&gt;主服务&lt;/h2&gt;

&lt;h3 id=&#34;单例模式&#34;&gt;单例模式&lt;/h3&gt;

&lt;p&gt;确定采用的技术后, 面临的需求如下:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开发要快, 但是功能模型还要超越之前的版本, 只有大约三个月的时间&lt;/li&gt;
&lt;li&gt;系统效率要足够, 可能有几十万人同时在线&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了满足这两个需求, 系统第一版采用了内存模型, 在开发速度和性能的要求都达到了需求, 系统如期上线.&lt;/p&gt;

&lt;h3 id=&#34;动静分离&#34;&gt;动静分离&lt;/h3&gt;

&lt;p&gt;之前版本将所有的请求揽于一身, 包括静态资源的处理, 这部分消耗了系统很多的资源, 可以由成熟的方案做专门的优化处理.&lt;/p&gt;

&lt;h4 id=&#34;1-web静态资源文件&#34;&gt;1. Web静态资源文件&lt;/h4&gt;

&lt;p&gt;静态文件分离的第一步就是将Web所需的静态资源简单的分离出来, 以方便对静态文件的请求进行单独处理.&lt;/p&gt;

&lt;p&gt;通过将静态文件存放在单独的服务器上, 并通过 nginx 作为文件服务器后, 完成了初步的改造, 这样做的目的有两个:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;网页静态资源请求不再有主服务处理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对静态文件服务进行了测试, 为接下来的系统静态数据分离做出可行性测试&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;2-静态数据文件&#34;&gt;2. 静态数据文件&lt;/h4&gt;

&lt;p&gt;系统运行本身也会产生很多的静态文件数据,  数据量增大后, 也会极大影响系统资源的分配. 这些服务相对来说复杂了许多, 无法通过简单的文件拆分完成.&lt;/p&gt;

&lt;p&gt;举例来说, 系统对图书的服务包含了三个方面:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;上传图书文件&lt;/li&gt;
&lt;li&gt;对上传的图书文件进行处理, 例如加密和修改部分文件&lt;/li&gt;
&lt;li&gt;向终端提供下载图书文件服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之前三个方面的服务全部由系统本身处理, 流程相对简单. 为了将图书文件作为静态文件单独处理, 新的流程将上传图书的流程分为两步:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;将图书文件本身上传到静态文件服务器, 并进行相应的逻辑处理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将上传成功的标记作为参数提交到系统主服务, 主服务只负责数据处理&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为了完成第一个流程, 需要创建新的上传服务, 该服务接收文件的上传, 将文件处理后交由文件服务器提供文件下载服务, 并返回文件路径信息.&lt;/p&gt;

&lt;p&gt;文件信息之后会被提交到主服务, 并由主服务分发给终端 App, 终端 App 最终能够获取到最新正确的文件地址, 从而整个服务流程完成.&lt;/p&gt;

&lt;p&gt;完成了图书文件的上传流程的改造后, 将用到的上传服务进行简单的扩展, 就可以将包括图书封面, 文章阅读等最终以静态文件存在的流程全部进行改造, 包括在线聊天(后面会专门讲到)这个对文件上传需求量最高的系统服务, 这样的结果就是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;系统主服务的负担进一步减轻&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;类似服务统一处理, 专门优化&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用上传服务的新功能的开发复杂度降低&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;系统模块化&#34;&gt;系统模块化&lt;/h3&gt;

&lt;p&gt;尽管静态文件相关的服务被剥离出来, 但是系统主服务还是太过庞大, 造成的影响包括:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;所有功能模块的集中, 导致开发修改任何功能都是对主服务的改动, 任何的微小 bug 都会导致主服务受影响&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于新成员, 无论开发功能还是修改 bug, 设计的代码范围大, 掌握系统设计的速度也比较慢&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;由于所有的资源都在同一个区域内, 功能开发很容易形成依赖, 如果开发成员的模块化意识差, 很容易写出混乱的代码, 给维护带来较大麻烦&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这些问题, 需要逐渐的将主服务中的功能进行拆分, 最终形成独立的功能服务.&lt;/p&gt;

&lt;h3 id=&#34;大前后端的分离&#34;&gt;大前后端的分离&lt;/h3&gt;

&lt;p&gt;一直以来, 后台管理系统与后台的数据服务是集合在主服务中的, 这样导致的问题就是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;即使业务需求导致的页面变化非常微小, 也需要对主服务进行升级&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;前端开发需要搭建和主服务同样的系统才能进行开发, 增大了开发难度&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这两个问题, 对系统主服务中的前端部分拆分成单独的前端系统, 通过 API 的方式调用数据&lt;/p&gt;

&lt;h3 id=&#34;功能模块的拆分&#34;&gt;功能模块的拆分&lt;/h3&gt;

&lt;p&gt;功能服务独立化时面对的问题主要是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如何确定功能的边界&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;之前同一个服务进程内的依赖问题如何解决&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;模块之间的数据如何同步&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-功能边界的确定&#34;&gt;1.功能边界的确定&lt;/h4&gt;

&lt;p&gt;这个需要从两个方面考虑:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;总结出业务特点, 渐渐划清了模块的大体边界&lt;/li&gt;
&lt;li&gt;根据业务的变化及时调整&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-服务的数据依赖与同步&#34;&gt;2.服务的数据依赖与同步&lt;/h4&gt;

&lt;p&gt;将原来服务切分进行拆分, 之前简单的在内存中查找数据就可以完成的操作, 现在划分为多个流程的组合, 很多问题就会产生, 比如说中间出错怎么处理, 性能降低较大.&lt;/p&gt;

&lt;p&gt;直觉的处理方案就是技术替代. 这方面有很多技术上的解决方案, 比如 内部 Restful API, RPC 等等, 用这些技术手段替代之前的内存查找, 最起码从表面上完成了拆分.&lt;/p&gt;

&lt;p&gt;但是, 如果解决只是这个思路进行拆分, 我觉得有很大的浪费. 因为模块的划分, 本来就是从底层逼迫架构设计者考虑之前数据调用的合理性. 如果能够通过模块的划分, 将原来隐藏的问题暴露出来, 通过重新思考, 架构方案的改进从而对整个系统加以改进, 那对于系统的清晰和可维护性一定是个巨大的进步.&lt;/p&gt;

&lt;p&gt;对于模块独立后, 对于必须依赖的数据的同步, 我们采用的方案是中间件事件提醒, 然后从数据库查询同步的方案. 具体来说, 每一个模块都有自己的缓存数据, 每个模块都可能具有读和写的操作, 如果发生写操作, 那么在数据成功写入到数据库完成持久化以后, 需要发送一个对该数据发生变化的事件提醒到中间件, 依赖该数据的模块通过订阅该事件获得提醒, 之后通过查询数据库中的相关数据, 更新缓存数据.&lt;/p&gt;

&lt;h4 id=&#34;3-成果&#34;&gt;3.成果&lt;/h4&gt;

&lt;p&gt;系统模块化之后, 不仅解决了之前存在的问题, 还额外得到了另外一些好处:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;由于模块之间的数据依赖, 促使系统不断的向无状态, 内聚的方向进化, 系统之间的耦合性降低, 整个系统更加清晰, 为之后功能的开发提供了坚实的基础&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;由于每个模块负责管理自己的数据, 但整个系统对外提供的 API 需要提供跨模块的数据, 促使缓存层的单独设计, 同时将终端和后台的 API 进行了分类处理&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;系统的无缝升级&#34;&gt;系统的无缝升级&lt;/h3&gt;

&lt;p&gt;好长时间以来, 系统的升级维护工作都需要暂停服务, 暂停服务从公司业务管理制度上需要经历一系列的流程, 流程的繁杂会阻碍功能的及时升级, 这样会造成很多问题:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;功能不能快速上线, 会造成功能的堆积, 堆积增多与升级出错的概率成正比, 导致系统升级出现意外的可能性增大&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;功能不能及时上线, 功能的反馈及时性降低, 不能及时跟进客户真正需求, 时间成本增大&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;功能不能上线, 依据该功能的后续开发工作不确定性增大, 开发成本升高&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;体验差&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;造成这个问题的主要原因在于系统的缓存使用的是内存, 功能升级需要重新加载数据, 而多个实例内存级别实时数据同步的成本较高.&lt;/p&gt;

&lt;p&gt;为了解决这个问题, 系统模块中的缓存逐渐用 redis 替代, 通过将缓存与功能分离, 可以同时启动同一模块的多个实例, 通过负载均衡的配置, 可以逐步用升级版本的实例代替就实例, 实现无缝升级.&lt;/p&gt;

&lt;p&gt;另外一个好处就是, 再将模块缓存与功能分离后, 顺带实现了系统的横向扩展, 系统的承载量不再是问题&lt;/p&gt;

&lt;h3 id=&#34;向数据平台的演进&#34;&gt;向数据平台的演进&lt;/h3&gt;

&lt;p&gt;经过之前的改进, 整个系统在业务上基本不存在大的问题. 随着系统的增大, 和第三方资源的对接成为主要考虑的方向, 不仅包括接入第三方的数据资源, 也包括开发资源, 如何向第三方开放数据接口, 增大系统的功能扩展能力摆在面前.&lt;/p&gt;

&lt;p&gt;解决思路是整个系统向平台化演进. 将内部的各个模块在现有的业务层和数据层的基础上, 从纵向上进一步细化拆分, 业务层分为基础业务和组合业务, 数据层分为业务数据和基础数据, 每个层次上开放合适的 API服务, 第三方的开发能力在服务基础上, 就可以快速得以实现.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>小书包在线聊天系统架构演进</title>
      <link>http://ssor.github.io/post/architechure_desigin_of_chat/</link>
      <pubDate>Wed, 05 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ssor.github.io/post/architechure_desigin_of_chat/</guid>
      <description>&lt;p&gt;介绍小书包聊天系统的架构历史及发展方向&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;技术方案&#34;&gt;技术方案&lt;/h2&gt;

&lt;p&gt;开发在线聊天功能考虑的第一个问题是: 能不能用第三方的系统?&lt;/p&gt;

&lt;p&gt;第三方的系统已经有成熟的商业解决方案, 自行开发成本很高. 但问题在于:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果经过第三方系统, 数据安全方面能不能得到保证&lt;/li&gt;
&lt;li&gt;如果要对聊天内容进行限制, 第三方系统是否能够做到&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;经过调研, 发现上面两个问题基本很难解决.&lt;/p&gt;

&lt;p&gt;那么能不能用开源的系统? 有许多具有很多应用案例的组合可以选择. 但是经过研究发现, 这些系统在部署和维护上也需要较大的投入, 再结合我们的技术选型和初期的产品需求, 发现自己开发一个反而成本较低, 而且后期也可以按照业务需求灵活改动.&lt;/p&gt;

&lt;h3 id=&#34;单例&#34;&gt;单例&lt;/h3&gt;

&lt;p&gt;初期的产品需求比较简单, 只要登录的人能够在群内互相发送消息即可, 不需要群的管理和消息的持久化&lt;/p&gt;

&lt;p&gt;所以第一版的在线聊天经过几天的开发就可以使用了. 它的功能包括:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户上线时, 访问服务, 服务根据用户属性, 将其分配到预设的群中&lt;/li&gt;
&lt;li&gt;用户发送消息, 该消息会通过 websocket 转发给群内的其他成员&lt;/li&gt;
&lt;li&gt;支持文字, 图片和语音&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一版在一台服务器上满足了十万人左右的聊天需求&lt;/p&gt;

&lt;h3 id=&#34;多节点&#34;&gt;多节点&lt;/h3&gt;

&lt;p&gt;用户量的增多对系统提出了更多的挑战, 主要有以下几个:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户量增大, 单个服务器难以承载&lt;/li&gt;
&lt;li&gt;登录过程较慢, 系统升级造成服务的中断&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这些问题, 系统在架构上进行以下的改进:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;预先创建群&lt;/li&gt;
&lt;li&gt;分配服务器节点和聊天功能进行模块上的分离, 分别由不同的服务负责, 各个模块可以单独部署&lt;/li&gt;
&lt;li&gt;用户登录采用两段式, 即先获取分配服务器节点, 通过分配服务器获取登录节点, 最终在分配的登录服务器节点上登录&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;改造完成后, 用户量承载方面已经没有问题, 由于多节点的支持, 用户登录也较为快速, 基本解决问题&lt;/p&gt;

&lt;h3 id=&#34;高可用&#34;&gt;高可用&lt;/h3&gt;

&lt;p&gt;随着用户分布范围的扩大, 遇到的网络情况变得复杂起来; 同时对服务的可用性也提出了更高的要求. 这种情况下需要解决的问题有:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务器分配节点的服务器失效&lt;/li&gt;
&lt;li&gt;终端对外网络有限制, 无法连接到服务器&lt;/li&gt;
&lt;li&gt;用户分配的节点服务器失效或者系统升级重启&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决上述问题, 系统进行了如下改进:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;客户端路由和分配服务器多点
&amp;gt; 客户端进行第一段的登录请求时, 获取的将会是一组分配服务器地址, 客户端轮询服务器列表, 其中一个失效不会阻碍登录的进行&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;用户群在各个聊天节点的自动迁移, 解决了节点升级和失效的问题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;未来计划&#34;&gt;未来计划&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;用户消息的持久化&lt;/li&gt;
&lt;li&gt;用户群在节点间迁移, 消息自动同步&lt;/li&gt;
&lt;li&gt;跳出聊天的范畴, 成为一个数据传输管道&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>